{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16f52918-0b5a-4d5c-a7b8-6eedfd03dc64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# /src/data_migration.py\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Configuration\n",
    "def get_config():\n",
    "    \"\"\"\n",
    "    Fetch configuration from environment variables.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "       \"SOURCE_TABLE\" : os.getenv(\"SOURCE_TABLE\", \"de_dev.bds_feature_store.bds_sms_data_structured\"),\n",
    "       \"TARGET_TABLE\" : os.getenv(\"TARGET_TABLE\", \"ds_dev.sms_parsing.sms_filtered_data_previous_day\"),\n",
    "        \"LOG_FILE\": os.getenv(\"LOG_FILE\", \"data_migration.log\"),\n",
    "        \"LOG_LEVEL\": os.getenv(\"LOG_LEVEL\", \"INFO\").upper(),\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "class DataFilteration:\n",
    "    def __init__(self, source_table, target_table, spark_session):\n",
    "        self.source_table = source_table\n",
    "        self.target_table = target_table\n",
    "        self.spark = spark_session\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"\n",
    "        Setup logging configuration.\n",
    "        \"\"\"\n",
    "        config = get_config()\n",
    "        logging.basicConfig(\n",
    "            filename=config[\"LOG_FILE\"],\n",
    "            level=config[\"LOG_LEVEL\"],\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        )\n",
    "        logging.info(\"Logging setup complete.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_date(date_str):\n",
    "        \"\"\"\n",
    "        Validate if the date string is in the correct YYYY-MM-DD format.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            return True\n",
    "        except ValueError:\n",
    "            logging.error(f\"Invalid date format: {date_str}. Use YYYY-MM-DD format.\")\n",
    "            return False\n",
    "\n",
    "    def fetch_data(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Fetch data from the source table.\n",
    "        \"\"\"\n",
    "        print(f\"Fetching data from the source table: {self.source_table}\")\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "                SELECT * FROM {self.source_table}\n",
    "                WHERE fetch_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "                LIMIT 50\n",
    "            \"\"\"\n",
    "            logging.info(f\"Executing query: {query}\")\n",
    "            df = self.spark.sql(query)\n",
    "            logging.info(f\"Fetched {df.count()} records from the source table: {self.source_table}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching data: {e}\")\n",
    "            raise RuntimeError(f\"Error fetching data: {e}\")\n",
    "\n",
    "    def write_data(self, df):\n",
    "        \"\"\"\n",
    "        Write data to the target table.\n",
    "        \"\"\"\n",
    "        print(f\"Writing data to the target table: {self.target_table}\")\n",
    "        try:\n",
    "            if df.count() > 0:\n",
    "                df.createOrReplaceTempView(\"temp_view\")\n",
    "                self.spark.sql(f\"\"\"\n",
    "                    INSERT OVERWRITE TABLE {self.target_table}\n",
    "                    SELECT * FROM temp_view\n",
    "                \"\"\")\n",
    "                logging.info(f\"Data written to target table: {self.target_table} successfully.\")\n",
    "            else:\n",
    "                logging.warning(\"No data to write; DataFrame is empty.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing data to the target table: {e}\")\n",
    "            raise RuntimeError(f\"Error writing data to the target table: {e}\")\n",
    "\n",
    "    def run(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Run the data filteration process.\n",
    "        \"\"\"\n",
    "        print(f\"running for date {start_date} to {end_date}\")\n",
    "        try:\n",
    "            # Validate dates\n",
    "            if not (self.validate_date(start_date) and self.validate_date(end_date)):\n",
    "                logging.error(\"Invalid date(s) provided. Exiting.\")\n",
    "                return\n",
    "\n",
    "            logging.info(\"Data filteration process started.\")\n",
    "\n",
    "            # Fetch data\n",
    "            data = self.fetch_data(start_date, end_date)\n",
    "\n",
    "            # Write data\n",
    "            self.write_data(data)\n",
    "\n",
    "            logging.info(\"Data filteration  process completed successfully.\")\n",
    "            print(f\"Done first step: {start_date} to {end_date}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data filteration  process failed: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "class UniqueTemplateExtractor:\n",
    "    def __init__(self, threshold=0.90):\n",
    "        \"\"\"Initialize the extractor with the specified threshold and embedding model.\"\"\"\n",
    "        try:\n",
    "            self.model = SentenceTransformer('BAAI/bge-base-en-v1.5')  # Using a pre-trained model for embeddings\n",
    "            self.threshold = threshold\n",
    "\n",
    "            # Dictionaries to store sender data\n",
    "            self.sender_indices = {}\n",
    "            self.sender_templates = {}\n",
    "            self.sender_template_ids = {}\n",
    "            self.sender_embeddings = {}\n",
    "\n",
    "            logging.info(\"Initialized UniqueTemplateExtractor successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing UniqueTemplateExtractor: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"Compute embedding of a given text message.\"\"\"\n",
    "        try:\n",
    "            embedding = self.model.encode(text, convert_to_tensor=True, normalize_embeddings=True)\n",
    "            return embedding.cpu().numpy()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating embedding for text '{text}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_template(self, sender_id, embedding, text):\n",
    "        \"\"\"Add a new template embedding to the FAISS index for the given sender.\"\"\"\n",
    "        try:\n",
    "            if sender_id not in self.sender_indices:\n",
    "                # Initialize FAISS index for new sender\n",
    "                dimension = self.model.get_sentence_embedding_dimension()\n",
    "                self.sender_indices[sender_id] = faiss.IndexFlatIP(dimension)\n",
    "                self.sender_templates[sender_id] = []\n",
    "                self.sender_template_ids[sender_id] = []\n",
    "                self.sender_embeddings[sender_id] = []\n",
    "\n",
    "            # Add to FAISS index and metadata for this sender\n",
    "            self.sender_indices[sender_id].add(embedding)\n",
    "            self.sender_templates[sender_id].append(text)\n",
    "            self.sender_template_ids[sender_id].append(len(self.sender_template_ids[sender_id]) + 1)\n",
    "            self.sender_embeddings[sender_id].append(embedding)\n",
    "\n",
    "            logging.info(f\"Template added for sender '{sender_id}': {text}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error adding template for sender '{sender_id}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def is_unique_template(self, sender_id, embedding):\n",
    "        \"\"\"Check if the embedding matches any existing template within the same sender.\"\"\"\n",
    "        try:\n",
    "            if sender_id not in self.sender_indices or len(self.sender_templates[sender_id]) == 0:\n",
    "                return True  # If no templates for this sender yet, it's unique\n",
    "\n",
    "            # Query the FAISS index for the specific sender\n",
    "            D, I = self.sender_indices[sender_id].search(embedding, k=1)\n",
    "\n",
    "            # D is the similarity score, I is the index of the nearest neighbor\n",
    "            if D[0][0] >= self.threshold:\n",
    "                return False  # Not unique, matches an existing template for this sender\n",
    "            return True  # Unique template within the sender\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error checking uniqueness for sender '{sender_id}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_unique_templates(self, messages):\n",
    "        \"\"\"Process messages and extract unique templates, checking within the same sender.\"\"\"\n",
    "        unique_templates = []\n",
    "        try:\n",
    "            for sender_id, msg in messages:\n",
    "                embedding = self.get_embedding(msg).reshape(1, -1)\n",
    "\n",
    "                # Check and add template within the sender's context\n",
    "                if self.is_unique_template(sender_id, embedding):\n",
    "                    self.add_template(sender_id, embedding, msg)\n",
    "\n",
    "            # Collect all unique templates across all senders\n",
    "            for sender_id in self.sender_templates:\n",
    "                templates = list(\n",
    "                    zip(\n",
    "                        self.sender_template_ids[sender_id],\n",
    "                        self.sender_templates[sender_id],\n",
    "                        self.sender_embeddings[sender_id],\n",
    "                    )\n",
    "                )\n",
    "                unique_templates.extend(\n",
    "                    [\n",
    "                        (sender_id, template_id, template_text, embedding.tolist())\n",
    "                        for template_id, template_text, embedding in templates\n",
    "                    ]\n",
    "                )\n",
    "            logging.info(f\"Extracted {len(unique_templates)} unique templates.\")\n",
    "            return unique_templates\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting unique templates: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_date, current_timestamp, udf, max as max_, row_number\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Configure logging\n",
    "log_path = f\"/Workspace/Users/nareshkumar.y@angelbroking.com/SMS parsing/hackathon/log_{datetime.now().strftime('%Y%m%d')}.log\"\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Define the UDF outside the class\n",
    "def id_sender_concept_reqired(sender):\n",
    "    sender = str(sender)\n",
    "    return not sender.isdigit()\n",
    "\n",
    "id_sender_concept_reqired_udf = udf(id_sender_concept_reqired, BooleanType())\n",
    "\n",
    "class SenderDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.DB_TABLE = \"ds_dev.sms_parsing.sms_filtered_data_previous_day\"\n",
    "        self.TARGET_DB_TABLE = \"ds_dev.sms_parsing.unique_sender_schema\"\n",
    "\n",
    "        self.FLAG = True\n",
    "\n",
    "        self.START_DATE = \"2024-01-01\"\n",
    "        self.END_DATE = \"2024-01-02\"\n",
    "        self.FIRST_DATE_FETCH = \"2024-01-01\"\n",
    "\n",
    "        self.NO_OF_DAYS_UPDATE = 15\n",
    "\n",
    "        # Initiate the spark session for script\n",
    "        self.spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"PySpark Session for sender job\") \\\n",
    "            .config(\"spark.some.config.options\", \"some-value\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def get_unique_new_senders(self, date_from='2023-01-01', min_count=5, start_date='2023-01-01', end_date='2023-01-01'):\n",
    "        query = f\"\"\"\n",
    "        WITH first_occurrences AS (\n",
    "            SELECT\n",
    "                sender,\n",
    "                MIN(fetch_date) AS first_fetch_date\n",
    "            FROM\n",
    "                {self.DB_TABLE}\n",
    "            GROUP BY\n",
    "                sender\n",
    "    ),\n",
    "    filtered_data AS (\n",
    "        SELECT\n",
    "            fetch_date,\n",
    "            sender,\n",
    "            COUNT(*) as count \n",
    "        FROM\n",
    "            {self.DB_TABLE}\n",
    "        GROUP BY\n",
    "            fetch_date,\n",
    "            sender\n",
    "    )\n",
    "    SELECT\n",
    "        DATE(fd.fetch_date) AS fetch_day,\n",
    "        fd.sender,\n",
    "        fd.count,\n",
    "        fo.first_fetch_date,\n",
    "        (CASE WHEN DATE(fd.fetch_date) == DATE(fo.first_fetch_date) THEN TRUE ELSE FALSE END) AS new_sender\n",
    "    FROM\n",
    "        filtered_data fd\n",
    "    JOIN\n",
    "        first_occurrences fo ON fd.sender = fo.sender\n",
    "    WHERE\n",
    "        fd.count > {min_count}\n",
    "        AND DATE(fd.fetch_date) = DATE(fo.first_fetch_date)\n",
    "        \"\"\"\n",
    "\n",
    "        spd = self.spark.sql(query)\n",
    "        return spd\n",
    "\n",
    "    def get_todays_date(self):\n",
    "        ist = pytz.timezone('Asia/Kolkata')\n",
    "        now_ist = datetime.now(ist)\n",
    "        today_ist = now_ist.strftime('%Y-%m-%d')\n",
    "        return today_ist\n",
    "\n",
    "    def get_n_days_back_date(self, days=0):\n",
    "        ist = pytz.timezone('Asia/Kolkata')\n",
    "        now_ist = datetime.now(ist)\n",
    "        date_n_days_before = now_ist - timedelta(days=days)\n",
    "        date_n_days_before_str = date_n_days_before.strftime('%Y-%m-%d')\n",
    "        return date_n_days_before_str\n",
    "\n",
    "    def get_target_table_df(self, target_table_name=None):\n",
    "        if target_table_name is None:\n",
    "            target_table_name = self.TARGET_DB_TABLE\n",
    "        target_table_df = self.spark.table(target_table_name)\n",
    "        return target_table_df\n",
    "\n",
    "    def get_max_id_count(self, target_table_df=None):\n",
    "        if target_table_df is None:\n",
    "            target_table_df = self.spark.table(self.TARGET_DB_TABLE)\n",
    "\n",
    "        max_existing_id = target_table_df.agg(F.max(\"sender_id\")).collect()[0][0]\n",
    "        if max_existing_id is None:\n",
    "            max_existing_id = 0\n",
    "\n",
    "        return max_existing_id\n",
    "\n",
    "    def add_new_processed_columns(self, source_data_df):\n",
    "        updated_data_df = source_data_df.withColumn(\"date_created\", current_date()) \\\n",
    "            .withColumn(\"time_created\", current_timestamp()) \\\n",
    "            .withColumn(\"concept_required\", id_sender_concept_reqired_udf(F.col(\"sender\")))\n",
    "        return updated_data_df\n",
    "\n",
    "    def add_unique_sender_ids(self, updated_data_df, max_existing_id=99):\n",
    "        window_spec = Window.orderBy(F.monotonically_increasing_id())  # Create an ordered window specification\n",
    "        updated_data_df_with_id = updated_data_df.withColumn(\"sender_id\", F.row_number().over(window_spec) + max_existing_id)\n",
    "        return updated_data_df_with_id\n",
    "\n",
    "    def filtering_new_ids(self, updated_data_df=None, target_table_df=None):\n",
    "        if updated_data_df is None:\n",
    "            pass\n",
    "        if target_table_df is None:\n",
    "            target_table_df = self.spark.table(self.TARGET_DB_TABLE)\n",
    "        filtered_new_data_df = updated_data_df.join(target_table_df.select(\"sender\"), on=\"sender\", how=\"left_anti\")\n",
    "        return filtered_new_data_df\n",
    "\n",
    "    def fill_remaining_cols_with_empty_string(self, updated_data_df):\n",
    "        updated_data_df = updated_data_df.withColumn(\"description\", F.lit(\"\"))\n",
    "        updated_data_df = updated_data_df.withColumn(\"concept\", F.lit(\"\"))\n",
    "        return updated_data_df\n",
    "\n",
    "    def fill_concept_and_description_cols_with_values(self, updated_data_df, concept_df):\n",
    "        updated_data_df = updated_data_df.alias(\"udf\")\n",
    "        concept_df = concept_df.alias(\"cdf\")\n",
    "    \n",
    "        # Joining on `sender` and `SenderId` with a left join\n",
    "        joined_df = updated_data_df.join(\n",
    "            concept_df,\n",
    "            updated_data_df[\"sender\"] == concept_df[\"SenderId\"],\n",
    "            \"left\"\n",
    "        )\n",
    "    \n",
    "        # Adding `Concept` and `description` columns from `concept_df`\n",
    "        result_df = joined_df.select(\n",
    "            \"udf.*\",  # Keep all original columns from updated_data_df\n",
    "            F.coalesce(F.col(\"cdf.description\"), F.lit(\"\")).alias(\"description\"),  # Add description with fallback\n",
    "            F.coalesce(F.col(\"cdf.concept\"), F.lit(\"\")).alias(\"concept\")  # Add Concept with fallback\n",
    "        )\n",
    "        return result_df\n",
    "\n",
    "    def get_type_casted_df(self, updated_data_df):\n",
    "        updated_data_df = updated_data_df.withColumn(\"sender_id\", updated_data_df[\"sender_id\"].cast(\"bigint\"))\n",
    "        updated_data_df = updated_data_df.withColumn(\"date_created\", to_date(col(\"date_created\"), \"yyyy-MM-dd\"))\n",
    "        updated_data_df = updated_data_df.withColumn(\"first_fetch_date\", to_date(col(\"first_fetch_date\"), \"yyyy-MM-dd\"))\n",
    "        print(updated_data_df.dtypes)\n",
    "        return updated_data_df\n",
    "\n",
    "    def filter_and_append_df_to_target_table(self, updated_data_df, target_table_name=None):\n",
    "        if target_table_name is None:\n",
    "            target_table_name = self.TARGET_DB_TABLE\n",
    "        updated_data_df = updated_data_df.select(\n",
    "            \"sender\", \"sender_id\", \"date_created\", \"first_fetch_date\", \"time_created\", \"description\", \"concept_required\", \"concept\")\n",
    "        updated_data_df.write.mode('append').saveAsTable(target_table_name)\n",
    "        return True\n",
    "\n",
    "    def fetch_concepts_for_senders(self, new_senders_df):\n",
    "        print(\"***Fetching concepts***\")\n",
    "        SENDERID_CONCEPTS_TABLE = \"ds_dev.dev_naman.senderid_themes\"\n",
    "        try:\n",
    "            # Get list of all sender IDs\n",
    "            sender_ids = [row['sender'] for row in new_senders_df.collect()]\n",
    "            if sender_ids:\n",
    "                # Fetch concepts for sender IDs and filter out any null concepts in the SQL query\n",
    "                concepts_df = self.spark.sql(f\"\"\"\n",
    "                    SELECT SenderId, Concept, `TRAI - Principal Entity Name` as description\n",
    "                    FROM {SENDERID_CONCEPTS_TABLE}\n",
    "                    WHERE SenderId IN ({', '.join([f\"'{id}'\" for id in sender_ids])}) AND Concept IS NOT NULL\n",
    "                \"\"\")\n",
    "                # log_info(f\"Fetched concepts for {concepts_df.count()} sender IDs (non-null concepts only).\")\n",
    "          \n",
    "                # Identify and log sender IDs without matching concepts or with null concepts\n",
    "                matched_sender_ids = [row['SenderId'] for row in concepts_df.collect()]\n",
    "                missing_concepts = set(sender_ids) - set(matched_sender_ids)\n",
    "                if missing_concepts:\n",
    "                    #log_info(f\"Missing concepts (either not found or null) for sender IDs: {missing_concepts}\")\n",
    "                    #log_info(f\"Missing concepts count: {len(missing_concepts)}\")\n",
    "                    print(f\"Missing concepts (either not found or null) for sender IDs: {missing_concepts}\")\n",
    "                    print(f\"Missing concepts count: {len(missing_concepts)}\")\n",
    "                display(concepts_df)\n",
    "                return concepts_df\n",
    "            else:\n",
    "                #log_info(\"No new sender IDs to process.\")\n",
    "                print(\"No new sender IDs to process.\")\n",
    "                return self.spark.createDataFrame([], schema=\"SenderId STRING, Concept STRING\")\n",
    "        except Exception as e:\n",
    "            #log_error(f\"Failed to fetch concepts for sender IDs: {e}\")\n",
    "            print(f\"Failed to fetch concepts for sender IDs: {e}\")\n",
    "            return self.spark.createDataFrame([], schema=\"SenderId STRING, Concept STRING\")\n",
    "\n",
    "    def process_data(self):\n",
    "        logging.info(\"Starting the job\")\n",
    "        res = False\n",
    "        source_df = None\n",
    "        if self.FLAG:\n",
    "            todays_date = self.get_todays_date()\n",
    "            start_date = self.FIRST_DATE_FETCH\n",
    "            from_date = self.FIRST_DATE_FETCH\n",
    "            min_c = 1\n",
    "            source_df = self.get_unique_new_senders(date_from=from_date, min_count=min_c, start_date=start_date, end_date=todays_date)\n",
    "        else:\n",
    "            todays_date = self.get_todays_date()\n",
    "            start_date = \"2024-01-01\"\n",
    "            from_date = self.get_n_days_back_date(days=self.NO_OF_DAYS_UPDATE)\n",
    "            min_c = 1\n",
    "            source_df = self.get_unique_new_senders(date_from=from_date, min_count=min_c, start_date=start_date, end_date=todays_date)\n",
    "\n",
    "        target_table_df = self.get_target_table_df(target_table_name=self.TARGET_DB_TABLE)\n",
    "        max_unique_id = self.get_max_id_count(target_table_df=target_table_df)\n",
    "        display(source_df)\n",
    "        print(f\"Count after fetching unique new senders: {source_df.count()}\")\n",
    "        updated_data_df = self.add_new_processed_columns(source_df)\n",
    "        print(f\"Count after adding new processed columns: {updated_data_df.count()}\")\n",
    "        updated_data_df_1 = self.add_unique_sender_ids(updated_data_df, max_existing_id=max_unique_id)\n",
    "        print(f\"Count after adding unique sender IDs: {updated_data_df_1.count()}\")\n",
    "        updated_data_df_2 = self.filtering_new_ids(updated_data_df_1, target_table_df=target_table_df)\n",
    "        print(f\"Count after filtering new IDs: {updated_data_df_2.count()}\")\n",
    "        new_senders_df = updated_data_df_2.select(\"sender\").distinct()\n",
    "        senderid_concept_df = self.fetch_concepts_for_senders(new_senders_df)\n",
    "        updated_data_df_3 = self.fill_concept_and_description_cols_with_values(updated_data_df_2, senderid_concept_df)\n",
    "        print(f\"Count after filling concept and description columns: {updated_data_df_3.count()}\")\n",
    "        display(updated_data_df_3)\n",
    "        updated_data_df_4 = self.get_type_casted_df(updated_data_df_3)\n",
    "        print(f\"Count after type casting: {updated_data_df_4.count()}\")\n",
    "        res = self.filter_and_append_df_to_target_table(updated_data_df=updated_data_df_4, target_table_name=self.TARGET_DB_TABLE)\n",
    "\n",
    "        if res:\n",
    "            logging.info(\"Job completed successfully and updated target table\")\n",
    "        else:\n",
    "            logging.info(\"Job failed to update target table\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    config = get_config()\n",
    "    spark = SparkSession.builder.appName(\"DataMigrationApp\").getOrCreate()\n",
    "\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: python data_migration.py <start_date> <end_date>\")\n",
    "        logging.error(\"Incorrect number of arguments provided.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    start_date = sys.argv[1]\n",
    "    end_date = sys.argv[2]\n",
    "\n",
    "    filteration = DataFilteration(\n",
    "        source_table=config[\"SOURCE_TABLE\"],\n",
    "        target_table=config[\"TARGET_TABLE\"],\n",
    "        spark_session=spark,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        #filteration.run(start_date, end_date)\n",
    "        filteration.run('2024-01-01','2024-01-01')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unhandled exception occurred: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    processor = SenderDataProcessor()\n",
    "    processor.process_data()\n",
    "\n",
    "    query = f\"\"\"\n",
    "                SELECT * FROM ds_dev.sms_parsing.sms_filtered_data_previous_day\n",
    "            \"\"\"\n",
    "    unique_sender_table=\"ds_dev.sms_parsing.unique_sender_schema\"\n",
    "    unique_pattern_table=\"ds_dev.sms_parsing.unique_sms_pattern_schema\"\n",
    "\n",
    "    logging.info(f\"Executing query: {query}\")\n",
    "    spark_df = spark .sql(query)\n",
    "    df = spark_df.toPandas()\n",
    "    print(df.head())\n",
    "    df=df[['sender','sms_text']]\n",
    "\n",
    "    messages = list(df.itertuples(index=False, name=None))\n",
    "    logging.info(f\"Loaded {len(messages)} messages from input  file.\")\n",
    "\n",
    "    # Initialize extractor\n",
    "    extractor = UniqueTemplateExtractor(threshold=0.90)\n",
    "    unique_templates = extractor.extract_unique_templates(messages)\n",
    "\n",
    "    # Save unique templates with embeddings to Excel\n",
    "    columns = ['sender', 'template_id', 'template_message', 'embedding']\n",
    "    unique_templates_df = pd.DataFrame(unique_templates, columns=columns)\n",
    "    unique_templates_df['template_attribute']='\"company:XXX\",\"date:XXX\"'\n",
    "    unique_templates_without_embedding_df=unique_templates_df[['sender', 'template_id', 'template_message', 'template_attribute']]\n",
    "    unique_templates_without_embedding_spark_df = spark.createDataFrame(unique_templates_without_embedding_df)\n",
    "\n",
    "    unique_templates_without_embedding_spark_df.createOrReplaceTempView(\"temp_df_view\")\n",
    "\n",
    "\n",
    "    query = \"\"\"\n",
    "    INSERT INTO ds_dev.sms_parsing.unique_sms_pattern_schema \n",
    "        SELECT *\n",
    "    FROM ds_dev.sms_parsing.unique_sender_schema st \n",
    "    INNER JOIN temp_df_view tt\n",
    "    on st.sender = tt.sender  \n",
    "    INNER JOIN ds_dev.sms_parsing.unique_sms_pattern_schema pt\n",
    "    ON st.sender_id = pt.sender_id\n",
    "    \"\"\"\n",
    "    #spark.sql(query)\n",
    "    spark.createDataFrame(unique_templates_df).createOrReplaceTempView(\"temp_df_view1\")\n",
    "    spark.sql(f\"\"\"\n",
    "                INSERT INTO ds_dev.sms_parsing.unique_template_embedding\n",
    "                SELECT * FROM temp_df_view1\n",
    "            \"\"\")\n",
    "    \n",
    "    unique_templates_df.to_excel('unique_template_data_with_embeddings_final.xlsx', index=False)\n",
    "\n",
    "    # Display unique templates\n",
    "    logging.info(\"Extracted Unique Templates:\")\n",
    "    for sender_id, template_id, template_text, embedding in unique_templates:\n",
    "        logging.info(f\"Sender: {sender_id}, Template ID: {template_id}, Text: {template_text}, Embedding: {embedding}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during processing: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sms_template_storage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
