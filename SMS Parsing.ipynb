{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81bae8c9-db66-4892-8e48-b9a078a98b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: faiss-cpu in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6d332f3a-93a4-410f-8603-33cf4f5fcff2/lib/python3.10/site-packages (1.9.0.post1)\nCollecting openpyxl\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 250.9/250.9 kB 20.6 MB/s eta 0:00:00\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6d332f3a-93a4-410f-8603-33cf4f5fcff2/lib/python3.10/site-packages (from faiss-cpu) (2.1.3)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from faiss-cpu) (23.2)\nCollecting et-xmlfile\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2746d7-d0a3-4689-bb2f-8ba5f4e96789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c9df66185b4259810f9860c62575d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sender_id  ...                                          embedding\n0    ACKOGI  ...  [[0.033985212445259094, -0.025274984538555145,...\n1    ACKOGI  ...  [[0.022043786942958832, 0.009573428891599178, ...\n2    ACKOGI  ...  [[0.005984822288155556, -0.02892979420721531, ...\n3    ACKOGI  ...  [[-0.04338106885552406, -0.025429215282201767,...\n4    ACKOGI  ...  [[0.03569294884800911, -0.05052488297224045, 0...\n\n[5 rows x 5 columns]\ndimension 1\nEmbedding dimension: 1\nReshaped embeddings: (12, 768)\nFAISS index created successfully.\nembedding type <class 'numpy.ndarray'>\nembedding shape (12, 768)\nEmbeddings added to FAISS index.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n/databricks/python/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Output:----- {\"insurance_number\":'GJ05JM8618\",\"insurance_amount\":\"Rs 5356.0\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "KB_FILE = r\"Knowledge_base.xlsx\"  # Path to the KB file\n",
    "# if not os.path.exists(KB_FILE):\n",
    "#     pd.DataFrame(columns=['template_ID', 'Embedding', 'Original SMS', 'Extracted Key-Value Pair']).to_excel(KB_FILE, index=False)\n",
    "EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\"  # Pre-trained model for embedding\n",
    "MODEL_NAME = \"microsoft/phi-3-mini-4k-Instruct\"  # Hugging Face model name\n",
    "TOP_K = 3  # Number of similar matches to retrieve\n",
    "\n",
    "# Initialize Embedding Model and Hugging Face Model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"cpu\", #cuda\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Step 1: Load Knowledge Base\n",
    "def load_knowledge_base(kb_file):\n",
    "    kb_df = pd.read_excel(kb_file)\n",
    "    print(kb_df.head())\n",
    "    embeddings = kb_df['embedding'].apply(lambda x: json.loads(x)).tolist()\n",
    "    sms_data = kb_df[['template_id','template_text','Extracted_key_value_pair']]\n",
    "    return np.array(embeddings), sms_data\n",
    "\n",
    "# Step 2: Compute Embedding for New SMS\n",
    "def compute_embedding(sms_text):\n",
    "    return embedding_model.encode([sms_text])[0]\n",
    "\n",
    "# Step 3: Build and Query FAISS Index\n",
    "def build_faiss_index(embeddings):\n",
    "    dimension = len(embeddings[0])\n",
    "    print(\"dimension\",dimension)\n",
    "    try:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "    except IndexError:\n",
    "        print(\"Embeddings are not a 2D array.\")\n",
    "        raise\n",
    "    try:\n",
    "        # Reshape to (12, 768)\n",
    "        embeddings = embeddings.reshape(embeddings.shape[0], embeddings.shape[2])\n",
    "        print(\"Reshaped embeddings:\", embeddings.shape)\n",
    "\n",
    "        # Build FAISS index\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "        print(\"FAISS index created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating FAISS index: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        print(\"embedding type\",type(embeddings))  # Should be <class 'numpy.ndarray'>\n",
    "        print(\"embedding shape\",embeddings.shape)\n",
    "\n",
    "\n",
    "\n",
    "        index.add(embeddings)\n",
    "        print(\"Embeddings added to FAISS index.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding embeddings to FAISS index: {e}\")\n",
    "        raise\n",
    "    return index\n",
    "\n",
    "def query_faiss_index(index, embeddings, query_embedding, top_k):\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    return indices[0], distances[0]\n",
    "\n",
    "# Step 4: Construct Few-Shot Prompt\n",
    "def construct_prompt(retrieved_data, new_sms):\n",
    "    context = \"\"\n",
    "    for i, row in retrieved_data.iterrows():\n",
    "        context += f\"SMS: {row['template_text']} -> Extracted: {row['Extracted_key_value_pair']}\\n\"\n",
    "    prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "    New SMS: \"{new_sms}\"\n",
    "    Extract the key-value pairs from the given input based on the context and return only the key-value pairs in output \n",
    "    \"\"\"\n",
    "    return prompt.strip()\n",
    "#Extract and output the structured key-value pair.\n",
    "# Step 5: Pass Prompt to Hugging Face Model\n",
    "def query_huggingface_model(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cpu\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=180,\n",
    "        temperature=0.3,\n",
    "        do_sample=False\n",
    "    ) #num_beams=3\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Step 6: Update Knowledge Base\n",
    "def update_knowledge_base(kb_file, new_template_id, new_embedding, new_sms, extracted_kv):\n",
    "    new_entry = {\n",
    "        \"template_ID\": new_template_id,\n",
    "        \"Embedding\": json.dumps(new_embedding.tolist()),\n",
    "        \"Original SMS\": new_sms,\n",
    "        \"Extracted Key-Value Pair\": json.dumps(extracted_kv)\n",
    "    }\n",
    "    kb_df = pd.read_excel(kb_file)\n",
    "    kb_df = kb_df.append(new_entry, ignore_index=True)\n",
    "    kb_df.to_excel(kb_file, index=False)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(input_text):\n",
    "    # Regular expression to find and capture key-value pairs from the Extracted part\n",
    "    match = re.search(r'Extracted:\\s*(\\{.*\\})', input_text)\n",
    "\n",
    "    if match:\n",
    "        # Extract the key-value pairs (the JSON object)\n",
    "        extracted_json = match.group(1)\n",
    "        return extracted_json\n",
    "    else:\n",
    "        return \"No key-value pairs found.\"\n",
    "\n",
    "\n",
    "\n",
    "# Main Pipeline\n",
    "def rag_pipeline(new_sms):\n",
    "    # Load Knowledge Base\n",
    "    embeddings, sms_data = load_knowledge_base(KB_FILE)\n",
    "    # Compute Embedding for New SMS\n",
    "    new_sms_embedding = compute_embedding(new_sms)\n",
    "    \n",
    "    # Build FAISS Index\n",
    "    faiss_index = build_faiss_index(embeddings)\n",
    "    \n",
    "    # Query FAISS for Similar SMS\n",
    "    indices, _ = query_faiss_index(faiss_index, embeddings, new_sms_embedding, TOP_K)\n",
    "    retrieved_data = sms_data.iloc[indices]\n",
    "    \n",
    "    # Construct Few-Shot Prompt\n",
    "    prompt = construct_prompt(retrieved_data, new_sms)\n",
    "    #print(\"Generated Prompt:\", prompt)\n",
    "    \n",
    "    # Query Hugging Face Model\n",
    "    llm_output = query_huggingface_model(prompt)\n",
    "    #print(\"LLM Output:\", llm_output)\n",
    "    llm_output=clean_extracted_text(llm_output)\n",
    "    print(\"LLM Output:-----\", llm_output)\n",
    "\n",
    "    \n",
    "    # Update Knowledge Base (Optional)\n",
    "    # extracted_kv = json.loads(llm_output)  # Assuming LLM returns JSON\n",
    "    # new_template_id = sms_data['template_ID'].max() + 1\n",
    "    # update_knowledge_base(KB_FILE, new_template_id, new_sms_embedding, new_sms, extracted_kv)\n",
    "    # print(\"Knowledge Base updated successfully.\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    new_sms = \"You're covered!<NAME> we have received payment of Rs 506.0 for your car insurance GJ05JM0000. Download the ACKO app now<NAME> manage claims and renewals on the go <URL>\"\n",
    "    rag_pipeline(new_sms)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SMS Parsing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
