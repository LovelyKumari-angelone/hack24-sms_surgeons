{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81bae8c9-db66-4892-8e48-b9a078a98b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.5/27.5 MB 67.5 MB/s eta 0:00:00\nCollecting openpyxl\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 250.9/250.9 kB 57.8 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from faiss-cpu) (23.2)\nCollecting numpy<3.0,>=1.25.0\n  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/16.3 MB 94.3 MB/s eta 0:00:00\nCollecting et-xmlfile\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: numpy, et-xmlfile, openpyxl, faiss-cpu\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-c64eff93-1e44-4d87-9792-1d1bb8f293c6\n    Can't uninstall 'numpy'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.2.0 requires numpy<1.24,>=1.16.0, but you have numpy 2.1.3 which is incompatible.\ntensorflow-cpu 2.14.1 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.1.3 which is incompatible.\nscipy 1.10.0 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.1.3 which is incompatible.\nnumba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 2.1.3 which is incompatible.\nlangchain 0.0.348 requires numpy<2,>=1, but you have numpy 2.1.3 which is incompatible.\ndatabricks-feature-engineering 0.2.0 requires numpy<2,>=1.19.2, but you have numpy 2.1.3 which is incompatible.\nSuccessfully installed et-xmlfile-2.0.0 faiss-cpu-1.9.0.post1 numpy-2.1.3 openpyxl-3.1.5\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2746d7-d0a3-4689-bb2f-8ba5f4e96789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05406c9b1e3147a9845c015043664c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sender_id  ...                                          embedding\n0    ACKOGI  ...  [[0.033985212445259094, -0.025274984538555145,...\n1    ACKOGI  ...  [[0.022043786942958832, 0.009573428891599178, ...\n2    ACKOGI  ...  [[0.005984822288155556, -0.02892979420721531, ...\n3    ACKOGI  ...  [[-0.04338106885552406, -0.025429215282201767,...\n4    ACKOGI  ...  [[0.03569294884800911, -0.05052488297224045, 0...\n\n[5 rows x 5 columns]\nLLM Output:----- {\"insurance_number\":'GJ05JM8618\",\"insurance_amount\":\"Rs 5356.0\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "KB_FILE = r\"Knowledge_base.xlsx\"  # Path to the KB file\n",
    "EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\"  # Pre-trained model for embedding\n",
    "MODEL_NAME = \"microsoft/phi-3-mini-4k-Instruct\"  # Hugging Face model name\n",
    "TOP_K = 3  # Number of similar matches to retrieve\n",
    "\n",
    "# Initialize Embedding Model and Hugging Face Model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"cpu\", #cuda\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Step 1: Load Knowledge Base\n",
    "def load_knowledge_base(kb_file):\n",
    "    kb_df = pd.read_excel(kb_file)\n",
    "    print(kb_df.head())\n",
    "    embeddings = kb_df['embedding'].apply(lambda x: json.loads(x)).tolist()\n",
    "    sms_data = kb_df[['template_id','template_text','Extracted_key_value_pair']]\n",
    "    return np.array(embeddings), sms_data\n",
    "\n",
    "# Step 2: Compute Embedding for New SMS\n",
    "def compute_embedding(sms_text):\n",
    "    return embedding_model.encode([sms_text])[0]\n",
    "\n",
    "# Step 3: Build and Query FAISS Index\n",
    "def build_faiss_index(embeddings):\n",
    "    dimension = len(embeddings[0])\n",
    "    try:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "    except IndexError:\n",
    "        raise\n",
    "    try:\n",
    "        embeddings = embeddings.reshape(embeddings.shape[0], embeddings.shape[2])\n",
    "        # Build FAISS index\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "    except Exception as e:\n",
    "    \n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        index.add(embeddings)\n",
    "        \n",
    "    except Exception as e: \n",
    "        raise\n",
    "    return index\n",
    "\n",
    "def query_faiss_index(index, embeddings, query_embedding, top_k):\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_k)\n",
    "    return indices[0], distances[0]\n",
    "\n",
    "# Step 4: Construct Few-Shot Prompt\n",
    "def construct_prompt(retrieved_data, new_sms):\n",
    "    context = \"\"\n",
    "    for i, row in retrieved_data.iterrows():\n",
    "        context += f\"SMS: {row['template_text']} -> Extracted: {row['Extracted_key_value_pair']}\\n\"\n",
    "    prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "    New SMS: \"{new_sms}\"\n",
    "    Extract the key-value pairs from the given input based on the context and return only the key-value pairs in output \n",
    "    \"\"\"\n",
    "    return prompt.strip()\n",
    "#Extract and output the structured key-value pair.\n",
    "# Step 5: Pass Prompt to Hugging Face Model\n",
    "def query_huggingface_model(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cpu\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=180,\n",
    "        temperature=0.3,\n",
    "        do_sample=False\n",
    "    ) #num_beams=3\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Step 6: Update Knowledge Base\n",
    "def update_knowledge_base(kb_file, new_template_id, new_embedding, new_sms, extracted_kv):\n",
    "    new_entry = {\n",
    "        \"template_ID\": new_template_id,\n",
    "        \"Embedding\": json.dumps(new_embedding.tolist()),\n",
    "        \"Original SMS\": new_sms,\n",
    "        \"Extracted Key-Value Pair\": json.dumps(extracted_kv)\n",
    "    }\n",
    "    kb_df = pd.read_excel(kb_file)\n",
    "    kb_df = kb_df.append(new_entry, ignore_index=True)\n",
    "    kb_df.to_excel(kb_file, index=False)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(input_text):\n",
    "    # Regular expression to find and capture key-value pairs from the Extracted part\n",
    "    match = re.search(r'Extracted:\\s*(\\{.*\\})', input_text)\n",
    "\n",
    "    if match:\n",
    "        # Extract the key-value pairs (the JSON object)\n",
    "        extracted_json = match.group(1)\n",
    "        return extracted_json\n",
    "    else:\n",
    "        return \"No key-value pairs found.\"\n",
    "\n",
    "\n",
    "\n",
    "# Main Pipeline\n",
    "def rag_pipeline(new_sms):\n",
    "    # Load Knowledge Base\n",
    "    embeddings, sms_data = load_knowledge_base(KB_FILE)\n",
    "    # Compute Embedding for New SMS\n",
    "    new_sms_embedding = compute_embedding(new_sms)\n",
    "    \n",
    "    # Build FAISS Index\n",
    "    faiss_index = build_faiss_index(embeddings)\n",
    "    \n",
    "    # Query FAISS for Similar SMS\n",
    "    indices, _ = query_faiss_index(faiss_index, embeddings, new_sms_embedding, TOP_K)\n",
    "    retrieved_data = sms_data.iloc[indices]\n",
    "    \n",
    "    # Construct Few-Shot Prompt\n",
    "    prompt = construct_prompt(retrieved_data, new_sms)\n",
    "    #print(\"Generated Prompt:\", prompt)\n",
    "    \n",
    "    # Query Hugging Face Model\n",
    "    llm_output = query_huggingface_model(prompt)\n",
    "    #print(\"LLM Output:\", llm_output)\n",
    "    llm_output=clean_extracted_text(llm_output)\n",
    "    print(\"LLM Output:-----\", llm_output)\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    new_sms = \"You're covered!<NAME> we have received payment of Rs 506.0 for your car insurance GJ05JM0000. Download the ACKO app now<NAME> manage claims and renewals on the go <URL>\"\n",
    "    rag_pipeline(new_sms)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SMS Parsing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
